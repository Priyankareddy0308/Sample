{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Customer Lifetime Value</th>\n",
       "      <th>Response</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Education</th>\n",
       "      <th>Effective To Date</th>\n",
       "      <th>EmploymentStatus</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "      <th>Location Code</th>\n",
       "      <th>...</th>\n",
       "      <th>Months Since Policy Inception</th>\n",
       "      <th>Number of Open Complaints</th>\n",
       "      <th>Number of Policies</th>\n",
       "      <th>Policy Type</th>\n",
       "      <th>Policy</th>\n",
       "      <th>Renew Offer Type</th>\n",
       "      <th>Sales Channel</th>\n",
       "      <th>Total Claim Amount</th>\n",
       "      <th>Vehicle Class</th>\n",
       "      <th>Vehicle Size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BU79786</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2763.519279</td>\n",
       "      <td>No</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2/24/11</td>\n",
       "      <td>Employed</td>\n",
       "      <td>F</td>\n",
       "      <td>56274</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Corporate Auto</td>\n",
       "      <td>Corporate L3</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Agent</td>\n",
       "      <td>384.811147</td>\n",
       "      <td>Two-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QZ44356</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>6979.535903</td>\n",
       "      <td>No</td>\n",
       "      <td>Extended</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>1/31/11</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Personal Auto</td>\n",
       "      <td>Personal L3</td>\n",
       "      <td>Offer3</td>\n",
       "      <td>Agent</td>\n",
       "      <td>1131.464935</td>\n",
       "      <td>Four-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI49188</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>12887.431650</td>\n",
       "      <td>No</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2/19/11</td>\n",
       "      <td>Employed</td>\n",
       "      <td>F</td>\n",
       "      <td>48767</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Personal Auto</td>\n",
       "      <td>Personal L3</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Agent</td>\n",
       "      <td>566.472247</td>\n",
       "      <td>Two-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WW63253</th>\n",
       "      <td>California</td>\n",
       "      <td>7645.861827</td>\n",
       "      <td>No</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>1/20/11</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Corporate Auto</td>\n",
       "      <td>Corporate L2</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Call Center</td>\n",
       "      <td>529.881344</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HB64268</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2813.692575</td>\n",
       "      <td>No</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>2/3/11</td>\n",
       "      <td>Employed</td>\n",
       "      <td>M</td>\n",
       "      <td>43836</td>\n",
       "      <td>Rural</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Personal Auto</td>\n",
       "      <td>Personal L1</td>\n",
       "      <td>Offer1</td>\n",
       "      <td>Agent</td>\n",
       "      <td>138.130879</td>\n",
       "      <td>Four-Door Car</td>\n",
       "      <td>Medsize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               State  Customer Lifetime Value Response  Coverage Education  \\\n",
       "Customer                                                                     \n",
       "BU79786   Washington              2763.519279       No     Basic  Bachelor   \n",
       "QZ44356      Arizona              6979.535903       No  Extended  Bachelor   \n",
       "AI49188       Nevada             12887.431650       No   Premium  Bachelor   \n",
       "WW63253   California              7645.861827       No     Basic  Bachelor   \n",
       "HB64268   Washington              2813.692575       No     Basic  Bachelor   \n",
       "\n",
       "         Effective To Date EmploymentStatus Gender  Income Location Code  ...  \\\n",
       "Customer                                                                  ...   \n",
       "BU79786            2/24/11         Employed      F   56274      Suburban  ...   \n",
       "QZ44356            1/31/11       Unemployed      F       0      Suburban  ...   \n",
       "AI49188            2/19/11         Employed      F   48767      Suburban  ...   \n",
       "WW63253            1/20/11       Unemployed      M       0      Suburban  ...   \n",
       "HB64268             2/3/11         Employed      M   43836         Rural  ...   \n",
       "\n",
       "         Months Since Policy Inception  Number of Open Complaints  \\\n",
       "Customer                                                            \n",
       "BU79786                              5                          0   \n",
       "QZ44356                             42                          0   \n",
       "AI49188                             38                          0   \n",
       "WW63253                             65                          0   \n",
       "HB64268                             44                          0   \n",
       "\n",
       "          Number of Policies     Policy Type        Policy  Renew Offer Type  \\\n",
       "Customer                                                                       \n",
       "BU79786                    1  Corporate Auto  Corporate L3            Offer1   \n",
       "QZ44356                    8   Personal Auto   Personal L3            Offer3   \n",
       "AI49188                    2   Personal Auto   Personal L3            Offer1   \n",
       "WW63253                    7  Corporate Auto  Corporate L2            Offer1   \n",
       "HB64268                    1   Personal Auto   Personal L1            Offer1   \n",
       "\n",
       "         Sales Channel Total Claim Amount  Vehicle Class Vehicle Size  \n",
       "Customer                                                               \n",
       "BU79786          Agent         384.811147   Two-Door Car      Medsize  \n",
       "QZ44356          Agent        1131.464935  Four-Door Car      Medsize  \n",
       "AI49188          Agent         566.472247   Two-Door Car      Medsize  \n",
       "WW63253    Call Center         529.881344            SUV      Medsize  \n",
       "HB64268          Agent         138.130879  Four-Door Car      Medsize  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Customer-Value-Analysis.csv').set_index('Customer')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9134 entries, BU79786 to Y167826\n",
      "Data columns (total 23 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   State                          9134 non-null   object \n",
      " 1   Customer Lifetime Value        9134 non-null   float64\n",
      " 2   Response                       9134 non-null   object \n",
      " 3   Coverage                       9134 non-null   object \n",
      " 4   Education                      9134 non-null   object \n",
      " 5   Effective To Date              9134 non-null   object \n",
      " 6   EmploymentStatus               9134 non-null   object \n",
      " 7   Gender                         9134 non-null   object \n",
      " 8   Income                         9134 non-null   int64  \n",
      " 9   Location Code                  9134 non-null   object \n",
      " 10  Marital Status                 9134 non-null   object \n",
      " 11  Monthly Premium Auto           9134 non-null   int64  \n",
      " 12  Months Since Last Claim        9134 non-null   int64  \n",
      " 13  Months Since Policy Inception  9134 non-null   int64  \n",
      " 14  Number of Open Complaints      9134 non-null   int64  \n",
      " 15  Number of Policies             9134 non-null   int64  \n",
      " 16  Policy Type                    9134 non-null   object \n",
      " 17  Policy                         9134 non-null   object \n",
      " 18  Renew Offer Type               9134 non-null   object \n",
      " 19  Sales Channel                  9134 non-null   object \n",
      " 20  Total Claim Amount             9134 non-null   float64\n",
      " 21  Vehicle Class                  9134 non-null   object \n",
      " 22  Vehicle Size                   9134 non-null   object \n",
      "dtypes: float64(2), int64(6), object(15)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check out target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fde02b28e80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Response'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> the dataset is very unballanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you can upload the dataset into Prevision platform with no need of data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing not required while using prevision platform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Pipeline:\n",
    "With many data transformation steps it is recommanded to use Pipeline class provided by Scikit-learn that helps to make sequenced transformations in the right order. We can do that using the FeatureUnion estimator offered by scikit-learn. This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Response = df.Response.apply(lambda X : 0 if X == 'No' else 1)\n",
    "X = df.drop(['Response'], axis = 1)\n",
    "y = df.Response.apply(lambda X : 0 if X == 'No' else 1)\n",
    "\n",
    "cats = [var for var, var_type in X.dtypes.items() if var_type=='object']\n",
    "nums = [var for var in X.columns if var not in cats]\n",
    "#cats.remove('Customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#source code from : https://www.kaggle.com/schopenhacker75/complete-beginner-guide\n",
    "\n",
    "#Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector(BaseEstimator, TransformerMixin ):\n",
    "    #Class Constructor \n",
    "    def __init__( self, feature_names):\n",
    "        self._feature_names = feature_names \n",
    "        \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Method that describes what we need this transformer to do\n",
    "    def transform( self, X, y = None ):\n",
    "        return X[self._feature_names].values \n",
    "\n",
    "\n",
    "#Defining the steps in the categorical pipeline \n",
    "cat_pipeline = Pipeline( [ ( 'cat_selector', FeatureSelector(cats) ),\n",
    "                          ( 'one_hot_encoder', OneHotEncoder(sparse = False ) ) ] )\n",
    "    \n",
    "#Defining the steps in the numerical pipeline     \n",
    "num_pipeline = Pipeline([\n",
    "        ( 'num_selector', FeatureSelector(nums) ),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "\n",
    "#Combining numerical and categorical piepline into one full big pipeline horizontally \n",
    "#using FeatureUnion\n",
    "full_pipeline = FeatureUnion( transformer_list = [ ( 'num_pipeline', num_pipeline ),\n",
    "                                                  ( 'cat_pipeline', cat_pipeline )] \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories=[]\n",
    "#for k, l in ohe_dict.items():\n",
    "#    categories.append([f'{k}_{cat}' for cat in list(l)])\n",
    "#flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#categories = flatten(categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply data transforamation : \n",
    "1. `fit_tranfsorm()` in train dataset\n",
    "2. `transform()` the test subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "#fit and transform the custom transformer in train\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "# transform the test with the trainef tansformer\n",
    "X_test_processed = full_pipeline.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed training dataset shape (7307, 122)\n",
      "transformed training dataset shape (1827, 122)\n"
     ]
    }
   ],
   "source": [
    "print('transformed training dataset shape', X_train_processed.shape)\n",
    "print('transformed training dataset shape', X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to persist the transformer that will be needed to make future predictions:\n",
    "\n",
    "[here](#https://scikit-learn.org/stable/modules/model_persistence.html) a more complete post about model persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/transformer.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "#  we will persist the transformer for future use without having to retrain\n",
    "#https://scikit-learn.org/stable/modules/model_persistence.html\n",
    "dump(full_pipeline, f'../model/transformer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will also save the trained one hot encoding categories (we will use it later to display feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouna/projects/venv/lib/python3.6/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "cat_step = full_pipeline.get_params()['transformer_list'][-1][-1]\n",
    "ohe = cat_step.steps[-1][-1]\n",
    "ohe_categories =dict(zip(cats, ohe.categories_))\n",
    "output_path = '../model/ohe_categories.pkl'\n",
    "with open(output_path, 'wb') as output:\n",
    "        pickle.dump(ohe_categories, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection:\n",
    "We will test out different types of algorithms and evaluate the performances using both the cross validation and train/test elvaluation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"XGBoost\", \"Random Forest\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    GradientBoostingClassifier(),\n",
    "    RandomForestClassifier()]\n",
    "\n",
    "scores = {}\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    # Cross val prediction    \n",
    "    cv_preds = cross_val_predict(clf, X_train_processed, y_train, method='predict_proba')\n",
    "    cv_score = log_loss(y_train, cv_preds)\n",
    "    \n",
    "    # holdout data \n",
    "    clf.fit(X_train_processed, y_train)\n",
    "    hd_preds = clf.predict_proba(X_test_processed)\n",
    "    hd_score = log_loss(y_test, hd_preds)\n",
    "    \n",
    "    # append the scores\n",
    "    scores[name] = [cv_score, hd_score]\n",
    "    #store the model\n",
    "    dump(clf, f'../model/{name}.joblib') \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors => CV_AUC : 0.6210403921626958, Holdout_AUC:0.6183440196177838\n",
      "XGBoost => CV_AUC : 0.26475707897857265, Holdout_AUC:0.27057581541492454\n",
      "Random Forest => CV_AUC : 0.09860086499954261, Holdout_AUC:0.07818186978283587\n"
     ]
    }
   ],
   "source": [
    "for model, perf in scores.items():\n",
    "    print(f'{model} => CV_AUC : {perf[0]}, Holdout_AUC:{perf[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> Random Seems to perform well, let fine tune it\n",
    "\n",
    "### Model Fine-Tuning:\n",
    "Once we have setected the one that seems to perform better (here random forest), we need to fine tune it. The most commun way is GridSearchCV evaluate all the possible combinations of setteled hyperparameter values using cross-validation.\n",
    "\n",
    "For example, the following code searches for the best combination of hyperparameter values for the RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'n_estimators': [100, 200]},\n",
       "                         {'max_features': ['log2'],\n",
       "                          'n_estimators': [50, 100, 200]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 4],\n",
       "                          'n_estimators': [150, 300]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=make_scorer(log_loss), verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [100, 200]},\n",
    "    {'n_estimators': [50, 100, 200], 'max_features': ['log2']},\n",
    "    {'bootstrap': [False], 'n_estimators': [150, 300], 'max_features': [2, 4]},\n",
    "]\n",
    "# about how to use the scorer strategy for the grid search:\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring\n",
    "scorer = make_scorer(log_loss)\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(RF, param_grid, cv=5,\n",
    "                           scoring=scorer,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features=2,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False, 'max_features': 2, 'n_estimators': 300}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/best.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_best = grid_search.best_estimator_\n",
    "\n",
    "# Cross val prediction    \n",
    "cv_preds = cross_val_predict(sk_best, \n",
    "                             X_train_processed, y_train, \n",
    "                             method='predict_proba')\n",
    "best_cv_score = log_loss(y_train, cv_preds)\n",
    "\n",
    "# holdout data \n",
    "sk_best.fit(X_train_processed, y_train)\n",
    "hd_preds = sk_best.predict_proba(X_test_processed)\n",
    "best_hd_score = log_loss(y_test, hd_preds)\n",
    "\n",
    "\n",
    "#store the model\n",
    "dump(sk_best, f'../model/best.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'best => CV_AUC : {best_cv_score}, Holdout_AUC:{best_hd_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make single prediction:\n",
    "for our projetc we will use the model to simulate deffierent values of the features and make single prediction for each simulation: we proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87333333, 0.12666667]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of new enty\n",
    "new_entry = X_test.loc[X_test.index[-1]].to_frame()\n",
    "# pass it throug the pipeline\n",
    "new_entry_processed = full_pipeline.transform(new_entry.T)\n",
    "# make prediction\n",
    "sk_best.predict_proba(new_entry_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the argmax\n",
    "sk_best.predict(new_entry_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification performances\n",
    "Now we will save model performances corresponding to different metrics such as precision_score, recall_score, accuracy_score and f1_score, we will use it later to display a pretty bar chart in our dash app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Cross val prediction    \n",
    "cv_one_preds = cross_val_predict(sk_best, \n",
    "                             X_train_processed, y_train, \n",
    "                             method='predict')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = {'accuracy' : accuracy_score(y_train, cv_one_preds),\n",
    "       'precision': precision_score(y_train, cv_one_preds),\n",
    "       'recall' : recall_score(y_train, cv_one_preds),\n",
    "       'f1_score': f1_score(y_train, cv_one_preds)}\n",
    "\n",
    "# persist the result\n",
    "output_path = '../model/sk_best_performances.pkl'\n",
    "with open(output_path, 'wb') as output:\n",
    "        pickle.dump(perf, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\PRIYANKA\\\\Desktop\\\\IBM-Customer-Value-Dashboarding-main\\\\src'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Custom Transformer that extracts columns passed as argument to its constructor\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor\n",
    "    def __init__(self, feature_names):\n",
    "        self._feature_names = feature_names\n",
    "\n",
    "        # Return self nothing else to do here\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "        # Method that describes what we need this transformer to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self._feature_names].values\n",
    "\n",
    "def make_full_pipeline(df):\n",
    "    #df = pd.read_csv('../data/Customer-Value-Analysis.csv').set_index('Customer')\n",
    "    X = df.drop(['Response'], axis=1)\n",
    "    y = df.Response.apply(lambda X: 0 if X == 'No' else 1)\n",
    "\n",
    "    cats = [var for var, var_type in X.dtypes.items() if var_type == 'object']\n",
    "    nums = [var for var in X.columns if var not in cats]\n",
    "    # Defining the steps in the categorical pipeline\n",
    "    cat_pipeline = Pipeline([('cat_selector', FeatureSelector(cats)),\n",
    "                             ('one_hot_encoder', OneHotEncoder(sparse=False))])\n",
    "\n",
    "    # Defining the steps in the numerical pipeline\n",
    "    num_pipeline = Pipeline([\n",
    "        ('num_selector', FeatureSelector(nums)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    # Combining numerical and categorical piepline into one full big pipeline horizontally\n",
    "    # using FeatureUnion\n",
    "    full_pipeline = FeatureUnion(transformer_list=[('num_pipeline', num_pipeline),\n",
    "                                                   ('cat_pipeline', cat_pipeline)]\n",
    "                                 )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # fit and transform the custom transformer in train\n",
    "    _ = full_pipeline.fit_transform(X_train)\n",
    "    return full_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANKA\\anaconda3\\lib\\site-packages\\sklearn\\base.py:334: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator DecisionTreeClassifier from version 0.22.2.post1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\Users\\PRIYANKA\\anaconda3\\lib\\site-packages\\sklearn\\base.py:334: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator RandomForestClassifier from version 0.22.2.post1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# We start with the import of standard ML librairies\n",
    "import pandas as pd\n",
    "import math\n",
    "from joblib import load\n",
    "from data_processing import make_full_pipeline\n",
    "import pickle\n",
    "# We add all Plotly and Dash necessary librairies\n",
    "import dash\n",
    "import plotly.graph_objects as go\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "HOME_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(HOME_PATH, 'data')\n",
    "MODELS_PATH = os.path.join(HOME_PATH, 'model')\n",
    "ASSETS_PATH = os.path.join(HOME_PATH, 'assets')\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Customer-Value-Analysis.csv\")).set_index('Customer')\n",
    "sk_best = load(os.path.join(MODELS_PATH, 'best.joblib'))\n",
    "\n",
    "# full_pipeline = load(os.path.join(MODELS_PATH, 'transformer.joblib'))\n",
    "full_pipeline = make_full_pipeline(df)\n",
    "\n",
    "ohe_path = os.path.join(MODELS_PATH, 'ohe_categories.pkl')\n",
    "perfs_path = os.path.join(MODELS_PATH, 'sk_best_performances.pkl')\n",
    "\n",
    "with open(ohe_path, 'rb') as input:\n",
    "    ohe_categories = pickle.load(input)\n",
    "\n",
    "categories = []\n",
    "for k, l in ohe_categories.items():\n",
    "    categories.append([f'{k}_{cat}' for cat in list(l)])\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "categories = flatten(categories)\n",
    "\n",
    "with open(perfs_path, 'rb') as input:\n",
    "    perfs = pickle.load(input)\n",
    "# scaling\n",
    "\n",
    "cats = [var for var, var_type in df.dtypes.items() if var_type == 'object']\n",
    "nums = [var for var in df.columns if var not in cats]\n",
    "cats.remove('Response')\n",
    "\n",
    "TOP = 10\n",
    "# We create a DataFrame to store the features' importance and their corresponding label\n",
    "df_feature_importances = pd.DataFrame(sk_best.feature_importances_ * 100, columns=[\"Importance\"],\n",
    "                                      index=nums + categories)\n",
    "df_feature_importances = df_feature_importances.sort_values(\"Importance\", ascending=False)\n",
    "df_feature_importances = df_feature_importances.loc[df_feature_importances.index[:TOP]]\n",
    "\n",
    "# We create a Features Importance Bar Chart\n",
    "fig_features_importance = go.Figure()\n",
    "fig_features_importance.add_trace(go.Bar(x=df_feature_importances.index,\n",
    "                                         y=df_feature_importances[\"Importance\"],\n",
    "                                         marker_color='rgb(171, 226, 251)')\n",
    "                                  )\n",
    "fig_features_importance.update_layout(title_text='<b>Features Importance of the model<b>', title_x=0.5)\n",
    "\n",
    "# We create a Features perfomances Bar Chart\n",
    "fig_perfs = go.Figure()\n",
    "fig_perfs.add_trace(go.Bar(y=list(perfs.keys()),\n",
    "                           x=list(perfs.values()),\n",
    "                           marker_color='rgb(171, 226, 251)',\n",
    "                           orientation='h')\n",
    "                    )\n",
    "fig_perfs.update_layout(title_text='<b>Best Model Performances<b>', title_x=0.5)\n",
    "\n",
    "cat_children = []\n",
    "for var in cats:\n",
    "    # Categorical children\n",
    "    sorted_modalities = list(df[var].value_counts().index)\n",
    "    cat_children.append(html.H4(children=var))\n",
    "    cat_children.append(dcc.Dropdown(\n",
    "        id='{}-dropdown'.format(var),\n",
    "        options=[{'label': value, 'value': value} for value in sorted_modalities],\n",
    "        value=sorted_modalities[0]\n",
    "    ))\n",
    "\n",
    "linear_children = []\n",
    "for var in nums:\n",
    "    # linear children\n",
    "    linear_children.append(html.H4(children=var))\n",
    "    desc = df[var].describe()\n",
    "    linear_children.append(dcc.Slider(\n",
    "        id='{}-dropdown'.format(var),\n",
    "        min=math.floor(desc['min']),\n",
    "        max=round(desc['max']),\n",
    "        step=None,\n",
    "        value=round(desc['mean']),\n",
    "        marks={i: '{}°'.format(i) for i in\n",
    "               range(int(desc['min']), int(desc['max']) + 1, max(int((desc['std'] / 1.5)), 1))}\n",
    "    ))\n",
    "# The command below can be activated in a standard notebook to display the chart\n",
    "# fig_features_importance.show()\n",
    "\n",
    "\n",
    "app = dash.Dash(__name__,\n",
    "# external CSS stylesheets\n",
    " external_stylesheets = [\n",
    "     #\"https://raw.githack.com/Athena75/IBM-Customer-Value-Dashboarding/main/assets/style.css\",\n",
    "     \"https://rawcdn.githack.com/Athena75/IBM-Customer-Value-Dashboarding/df971ae38117d85c8512a72643ce6158cde7a4eb/assets/style.css\"\n",
    " ]\n",
    ")\n",
    "\n",
    "# We apply basic HTML formatting to the layout\n",
    "app.layout = html.Div(children=[\n",
    "    # first row : Title\n",
    "    html.Div(children=[\n",
    "        html.Div(children=[html.H1(children=\"Simulation Tool : IBM Customer Churn\")],\n",
    "                 className='title'),\n",
    "\n",
    "    ],\n",
    "        style={\"display\": \"block\"}),\n",
    "    # second row :\n",
    "    html.Div(children=[\n",
    "        # first column : fig feature importance + linear + prediction\n",
    "        html.Div(children=[\n",
    "            html.Div(children=[dcc.Graph(figure=fig_features_importance, className='graph')] + linear_children),\n",
    "            # prediction result\n",
    "            html.Div(children=[html.H2(children=\"Prediction:\"),\n",
    "                               html.H2(id=\"prediction_result\")],\n",
    "                     className='prediction')],\n",
    "                 className='column'),\n",
    "        # second column : fig performances categorical\n",
    "        html.Div(children=[dcc.Graph(figure=fig_perfs, className='graph')] + cat_children,\n",
    "                 className='column')\n",
    "    ],\n",
    "        className='row')\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "# The callback function will provide one \"Output\" in the form of a string (=children)\n",
    "@app.callback(Output(component_id=\"prediction_result\", component_property=\"children\"),\n",
    "              # The values corresponding to sliders and dropdowns of respectively numerical and categorical features\n",
    "              [Input('{}-dropdown'.format(var), 'value') for var in nums + cats])\n",
    "# The input variable are set in the same order as the callback Inputs\n",
    "def update_prediction(*X):\n",
    "    # get the data input and map it to the correponding feature names\n",
    "    payload = dict(zip(nums + cats, X))\n",
    "    # create one line dataframe\n",
    "    frame_X = pd.DataFrame(payload, index=[0])\n",
    "    # pass it through the pre-fitted transformer\n",
    "    X_processed = full_pipeline.transform(frame_X)\n",
    "\n",
    "    prediction = sk_best.predict_proba(X_processed)[0]\n",
    "\n",
    "    # And retuned to the Output of the callback function\n",
    "    return \" {}% No , {}% Yes\".format(\"%.2f\" % (prediction[0] * 100),\n",
    "                                      \"%.2f\" % (prediction[1] * 100))\n",
    "\n",
    "\n",
    "server = app.server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-728d9af8c47b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp_dash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.app_dash import server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
